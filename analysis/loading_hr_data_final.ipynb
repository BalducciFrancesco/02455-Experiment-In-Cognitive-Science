{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f28359d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22 participants from JSON\n",
      "Loaded HR data for participant 1: 1065 rows\n",
      "Loaded HR data for participant 10: 966 rows\n",
      "Loaded HR data for participant 11: 1490 rows\n",
      "Loaded HR data for participant 12: 1469 rows\n",
      "Loaded HR data for participant 13: 1570 rows\n",
      "Loaded HR data for participant 14: 1288 rows\n",
      "Loaded HR data for participant 15: 1637 rows\n",
      "Loaded HR data for participant 16: 971 rows\n",
      "Loaded HR data for participant 17: 971 rows\n",
      "Loaded HR data for participant 18: 1154 rows\n",
      "Loaded HR data for participant 19: 1154 rows\n",
      "Loaded HR data for participant 2: 914 rows\n",
      "Loaded HR data for participant 20: 971 rows\n",
      "Loaded HR data for participant 21: 897 rows\n",
      "Loaded HR data for participant 22: 1066 rows\n",
      "Loaded HR data for participant 23: 1152 rows\n",
      "Loaded HR data for participant 3: 1065 rows\n",
      "Loaded HR data for participant 4: 1268 rows\n",
      "Loaded HR data for participant 5: 1172 rows\n",
      "Loaded HR data for participant 6: 1254 rows\n",
      "Loaded HR data for participant 7: 1176 rows\n",
      "Loaded HR data for participant 8: 817 rows\n",
      "HR participants available: [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "Attached HR segments for participant 1\n",
      "Attached HR segments for participant 2\n",
      "Attached HR segments for participant 3\n",
      "Attached HR segments for participant 4\n",
      "Attached HR segments for participant 5\n",
      "Attached HR segments for participant 6\n",
      "Attached HR segments for participant 7\n",
      "Attached HR segments for participant 8\n",
      "Attached HR segments for participant 10\n",
      "Attached HR segments for participant 11\n",
      "Attached HR segments for participant 12\n",
      "Attached HR segments for participant 13\n",
      "Attached HR segments for participant 14\n",
      "Attached HR segments for participant 15\n",
      "Attached HR segments for participant 16\n",
      "Attached HR segments for participant 17\n",
      "Attached HR segments for participant 18\n",
      "Attached HR segments for participant 19\n",
      "Attached HR segments for participant 20\n",
      "Attached HR segments for participant 21\n",
      "Attached HR segments for participant 22\n",
      "Attached HR segments for participant 23\n",
      "Saved merged data with HR to experiment_with_heart_rate.json\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# ------------------------\n",
    "# 1. Helpers\n",
    "# ------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def load_hr_file(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a participantN.csv HR file into a DataFrame with:\n",
    "    RR, ArtifactCorrectedRR, RawArtifact, ts\n",
    "\n",
    "    Works even if there are extra header/meta lines.\n",
    "    \"\"\"\n",
    "    # Read whole file as text lines\n",
    "    lines = Path(path).read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "    # 1) Find the header line (contains RR and ts)\n",
    "    header_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"RR\" in line and \"ts\" in line:\n",
    "            header_idx = i\n",
    "            break\n",
    "\n",
    "    if header_idx is None:\n",
    "        raise ValueError(f\"Could not find header with 'RR' and 'ts' in {path}\")\n",
    "\n",
    "    header_line = lines[header_idx].strip()\n",
    "\n",
    "    # 2) Detect delimiter (comma or semicolon)\n",
    "    if \";\" in header_line and \",\" not in header_line:\n",
    "        delim = \";\"\n",
    "    else:\n",
    "        delim = \",\"\n",
    "\n",
    "    header = [h.strip() for h in header_line.split(delim)]\n",
    "\n",
    "    # 3) Parse data lines\n",
    "    data_rows = []\n",
    "    for line in lines[header_idx + 1 :]:\n",
    "        if not line.strip():\n",
    "            continue  # skip empty lines\n",
    "        parts = [p.strip() for p in line.split(delim)]\n",
    "        if len(parts) != len(header):\n",
    "            # skip lines that don't match the header shape (footers, meta, etc.)\n",
    "            continue\n",
    "        data_rows.append(parts)\n",
    "\n",
    "    # 4) Build DataFrame\n",
    "    df = pd.DataFrame(data_rows, columns=header)\n",
    "\n",
    "    # 5) Convert numeric columns safely\n",
    "    numeric_cols = [\"RR\", \"ArtifactCorrectedRR\", \"RawArtifact\"]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    if \"RawArtifact\" in df.columns:\n",
    "        df[\"RawArtifact\"] = df[\"RawArtifact\"].fillna(0).astype(int)\n",
    "\n",
    "    # 6) Parse timestamps\n",
    "    if \"ts\" in df.columns:\n",
    "        df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
    "\n",
    "    # 7) Drop rows without timestamp or RR\n",
    "    df = df.dropna(subset=[\"ts\", \"RR\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_exptimestamp(s: str) -> datetime:\n",
    "    \"\"\"\n",
    "    '2025-11-12 14h50.45.373178 +0100' -> aware datetime\n",
    "    \"\"\"\n",
    "    date, time_part, tz = s.split(\" \")\n",
    "    time_part = time_part.replace(\"h\", \":\")\n",
    "    time_part = time_part.replace(\".\", \":\", 1)  # only first '.' -> ':'\n",
    "    fixed = f\"{date} {time_part} {tz}\"\n",
    "    return datetime.strptime(fixed, \"%Y-%m-%d %H:%M:%S.%f %z\")\n",
    "\n",
    "\n",
    "def json_time_to_naive(s: str):\n",
    "    \"\"\"Convert experiment timestamp string to naive datetime (no tz).\"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    dt = parse_exptimestamp(s)\n",
    "    return dt.replace(tzinfo=None)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 2. Load experiment JSON\n",
    "# ------------------------\n",
    "\n",
    "experiment_json_path = \"experiment_data.json\"  # adjust if needed\n",
    "\n",
    "with open(experiment_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    participants = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(participants)} participants from JSON\")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 3. Load ALL heart-rate CSV files\n",
    "# ------------------------\n",
    "\n",
    "hr_folder = \"hr_data\"  # <- change this if your HR files are elsewhere\n",
    "hr_files = glob(os.path.join(hr_folder, \"participant*.csv\"))\n",
    "\n",
    "hr_data = {}  # dict: {participant_id: DataFrame}\n",
    "\n",
    "for path in hr_files:\n",
    "    base = os.path.basename(path)              # e.g. \"participant1.csv\"\n",
    "    num_str = base.replace(\"participant\", \"\").replace(\".csv\", \"\")\n",
    "    try:\n",
    "        pid = int(num_str)\n",
    "    except ValueError:\n",
    "        continue  # ignore weird files\n",
    "\n",
    "    df_hr = load_hr_file(path)\n",
    "    hr_data[pid] = df_hr\n",
    "    print(f\"Loaded HR data for participant {pid}: {len(df_hr)} rows\")\n",
    "\n",
    "print(\"HR participants available:\", sorted(hr_data.keys()))\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 4. Match HR windows to each video for each participant\n",
    "# ------------------------\n",
    "\n",
    "for p in participants:\n",
    "    pid = p.get(\"participant_id\") or p.get(\"id\")\n",
    "    if pid is None:\n",
    "        continue\n",
    "\n",
    "    # skip if we don't have HR for this participant\n",
    "    if pid not in hr_data:\n",
    "        print(f\"No HR data for participant {pid}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    df_hr = hr_data[pid].copy()\n",
    "    df_hr[\"ts\"] = pd.to_datetime(df_hr[\"ts\"])  # ensure datetime\n",
    "\n",
    "    for stim in p[\"stimuli\"]:\n",
    "        v_start_str = stim.get(\"video_start\")\n",
    "        v_end_str = stim.get(\"video_end\")\n",
    "\n",
    "        start = json_time_to_naive(v_start_str)\n",
    "        end = json_time_to_naive(v_end_str)\n",
    "\n",
    "        if start is None or end is None:\n",
    "            stim[\"heart_rate\"] = []\n",
    "            continue\n",
    "\n",
    "        # ALL HR samples between video_start and video_end\n",
    "        mask = (df_hr[\"ts\"] >= start) & (df_hr[\"ts\"] <= end)\n",
    "        seg = df_hr.loc[mask].reset_index(drop=True)\n",
    "\n",
    "        # store as list-of-dicts so it can go into JSON\n",
    "        stim[\"heart_rate\"] = seg.to_dict(orient=\"records\")\n",
    "\n",
    "    print(f\"Attached HR segments for participant {pid}\")\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 5. Save merged JSON\n",
    "# ------------------------\n",
    "\n",
    "output_path = \"experiment_with_heart_rate.json\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(participants, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Saved merged data with HR to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ActiveML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
