{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23be0c1a",
   "metadata": {},
   "source": [
    "# Extracting PsychoPy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42d71e",
   "metadata": {},
   "source": [
    "## Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b901ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def parse_expstart(exp_start_str: str) -> datetime:\n",
    "    \"\"\"\n",
    "    Convert '2025-11-12 14h45.49.380213 +0100'\n",
    "    -> datetime(2025-11-12 14:45:49.380213+01:00)\n",
    "    \"\"\"\n",
    "    date, time_part, tz = exp_start_str.split(\" \")\n",
    "\n",
    "    # 14h45.49.380213 -> 14:45:49.380213\n",
    "    time_part = time_part.replace(\"h\", \":\")\n",
    "    time_part = time_part.replace(\".\", \":\", 1)  # only first '.' -> ':'\n",
    "\n",
    "    fixed = f\"{date} {time_part} {tz}\"\n",
    "    return datetime.strptime(fixed, \"%Y-%m-%d %H:%M:%S.%f %z\")\n",
    "\n",
    "\n",
    "def offset_to_str(offset_seconds, base_dt):\n",
    "    \"\"\"Convert a float offset in seconds to same string format as expStart.\"\"\"\n",
    "    if base_dt is None or pd.isna(offset_seconds):\n",
    "        return None\n",
    "    dt = base_dt + timedelta(seconds=float(offset_seconds))\n",
    "    return dt.strftime(\"%Y-%m-%d %Hh%M.%S.%f %z\")\n",
    "\n",
    "\n",
    "def parse_participant_csv(path: str) -> dict:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # --- participant id ---\n",
    "    try:\n",
    "        participant_id = int(df[\"participant\"].dropna().iloc[0])\n",
    "    except Exception:\n",
    "        # fall back to filename prefix\n",
    "        participant_id = int(os.path.basename(path).split(\"_\", 1)[0])\n",
    "\n",
    "    # --- focus (all trials) ---\n",
    "    focus_ratings = []\n",
    "    if \"stimuli.focus_slider.response\" in df.columns:\n",
    "        focus_ratings = df[\"stimuli.focus_slider.response\"].dropna().tolist()\n",
    "    focus_mean = float(sum(focus_ratings) / len(focus_ratings)) if focus_ratings else None\n",
    "\n",
    "    # --- opinion (final slider) ---\n",
    "    if \"opinion_slider.response\" in df.columns and df[\"opinion_slider.response\"].notna().any():\n",
    "        opinion = float(df[\"opinion_slider.response\"].dropna().iloc[0])\n",
    "    else:\n",
    "        opinion = None\n",
    "\n",
    "    # --- experiment start/end timestamps ---\n",
    "    if \"expStart\" in df.columns and df[\"expStart\"].notna().any():\n",
    "        raw_start = df[\"expStart\"].dropna().iloc[0]\n",
    "        exp_start_dt = parse_expstart(raw_start)\n",
    "        exp_start = raw_start\n",
    "    else:\n",
    "        exp_start_dt = None\n",
    "        exp_start = None\n",
    "\n",
    "    if \"expEndTime\" in df.columns and df[\"expEndTime\"].notna().any() and exp_start_dt is not None:\n",
    "        end_epoch = float(df[\"expEndTime\"].dropna().iloc[0])\n",
    "        exp_end_dt = datetime.fromtimestamp(end_epoch, tz=timezone.utc).astimezone(\n",
    "            exp_start_dt.tzinfo\n",
    "        )\n",
    "        exp_end = exp_end_dt.strftime(\"%Y-%m-%d %Hh%M.%S.%f %z\")\n",
    "    else:\n",
    "        exp_end = None\n",
    "\n",
    "    # --- trial rows (one per video) ---\n",
    "    if \"video_index\" in df.columns:\n",
    "        trial_rows = df[df[\"video_index\"].notna()]\n",
    "    else:\n",
    "        trial_rows = pd.DataFrame()\n",
    "\n",
    "    stimuli = []\n",
    "\n",
    "    for _, row in trial_rows.iterrows():\n",
    "        # SAM ratings\n",
    "        sam_cols = [\"rating_sam_1.keys\", \"rating_sam_2.keys\", \"rating_sam_3.keys\"]\n",
    "        sam_values = []\n",
    "        for col in sam_cols:\n",
    "            if col in df.columns and not pd.isna(row[col]):\n",
    "                sam_values.append(float(row[col]))\n",
    "            else:\n",
    "                sam_values.append(None)\n",
    "        trusted = all(v is not None for v in sam_values)\n",
    "\n",
    "        # SAM timestamps (absolute times, based on r_SAM_* offsets)\n",
    "        timestamps = {\n",
    "            \"sam1_start\": offset_to_str(row.get(\"r_SAM_1.started\"), exp_start_dt),\n",
    "            \"sam1_end\":   offset_to_str(row.get(\"r_SAM_1.stopped\"), exp_start_dt),\n",
    "            \"sam2_start\": offset_to_str(row.get(\"r_SAM_2.started\"), exp_start_dt),\n",
    "            \"sam2_end\":   offset_to_str(row.get(\"r_SAM_2.stopped\"), exp_start_dt),\n",
    "            \"sam3_start\": offset_to_str(row.get(\"r_SAM_3.started\"), exp_start_dt),\n",
    "            \"sam3_end\":   offset_to_str(row.get(\"r_SAM_3.stopped\"), exp_start_dt),\n",
    "        }\n",
    "\n",
    "        # video start/end â€“ ONLY using video.started / video.stopped\n",
    "        video_start = offset_to_str(row.get(\"video.started\"), exp_start_dt)\n",
    "        video_end   = offset_to_str(row.get(\"r_video.stopped\"), exp_start_dt)\n",
    "\n",
    "        stim_struct = {\n",
    "            \"id\": int(row[\"video_index\"]),\n",
    "            \"video_id\": row[\"video_id\"],\n",
    "            \"video_start\": video_start,\n",
    "            \"video_end\": video_end,\n",
    "            \"video_response\": {\n",
    "                \"belief\": row.get(\"belief\"),\n",
    "                \"sam\": sam_values,\n",
    "                \"timestamps\": timestamps,\n",
    "                #\"trusted\": trusted,\n",
    "            },\n",
    "        }\n",
    "        stimuli.append(stim_struct)\n",
    "\n",
    "    return {\n",
    "        \"participant_id\": participant_id,\n",
    "        \"focus\": {\"mean\": focus_mean, \"all_ratings\": focus_ratings},\n",
    "        \"opinion\": opinion,\n",
    "        \"timestamp_start\": exp_start,\n",
    "        \"timestamp_end\": exp_end,\n",
    "        \"stimuli\": stimuli,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708c3952",
   "metadata": {},
   "source": [
    "## Looping over all CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f1fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "pattern = str(Path(\"..\") / \"trials_psychopy\" / \"*_manipulation-of-belief_*.csv\")\n",
    "\n",
    "all_participants = []\n",
    "for csv_path in glob(pattern):\n",
    "    all_participants.append(parse_participant_csv(csv_path))\n",
    "\n",
    "all_participants.sort(key=lambda p: p[\"participant_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fd429",
   "metadata": {},
   "source": [
    "## Check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e9f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def is_missing(v):\n",
    "    if v is None: return True\n",
    "    if isinstance(v, float) and math.isnan(v): return True\n",
    "    if isinstance(v, str) and v.strip().lower() in (\"\", \"nan\", \"none\", \"null\"): return True\n",
    "    return False\n",
    "\n",
    "def find_missing(obj, path=\"root\"):\n",
    "    missing = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items(): missing += find_missing(v, f\"{path}.{k}\")\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj): missing += find_missing(v, f\"{path}[{i}]\")\n",
    "    else:\n",
    "        if is_missing(obj): missing.append((path, obj))\n",
    "    return missing\n",
    "\n",
    "missing = find_missing(all_participants, \"root\")\n",
    "if not missing:\n",
    "    print(\"No missing values found.\")\n",
    "else:\n",
    "    print(f\"Found {len(missing)} missing values. Showing up to 200 entries:\\n\")\n",
    "    for path, val in missing[:200]: print(f\"{path} -> {repr(val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca85af",
   "metadata": {},
   "source": [
    "## Enrich with HR data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df640259",
   "metadata": {},
   "source": [
    "### Imports + robust HR loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4116ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_hr_file(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a participantN.csv HR file into a tidy DataFrame with columns:\n",
    "    ts (datetime), RR, ArtifactCorrectedRR, RawArtifact.\n",
    "    Works even if header isn't on the first line.\n",
    "    \"\"\"\n",
    "    lines = Path(path).read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "    # find header line (must contain both 'RR' and 'ts')\n",
    "    header_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"RR\" in line and \"ts\" in line:\n",
    "            header_idx = i\n",
    "            break\n",
    "    if header_idx is None:\n",
    "        raise ValueError(f\"Could not find header with 'RR' and 'ts' in {path}\")\n",
    "\n",
    "    header_line = lines[header_idx].strip()\n",
    "\n",
    "    # detect delimiter\n",
    "    if \";\" in header_line and \",\" not in header_line:\n",
    "        delim = \";\"\n",
    "    else:\n",
    "        delim = \",\"  # default\n",
    "\n",
    "    header = [h.strip() for h in header_line.split(delim)]\n",
    "\n",
    "    data_rows = []\n",
    "    for line in lines[header_idx + 1:]:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        parts = [p.strip() for p in line.split(delim)]\n",
    "        if len(parts) != len(header):\n",
    "            continue\n",
    "        data_rows.append(parts)\n",
    "\n",
    "    df = pd.DataFrame(data_rows, columns=header)\n",
    "\n",
    "    # numeric columns\n",
    "    for col in [\"RR\", \"ArtifactCorrectedRR\", \"RawArtifact\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    if \"RawArtifact\" in df.columns:\n",
    "        df[\"RawArtifact\"] = df[\"RawArtifact\"].fillna(0).astype(int)\n",
    "\n",
    "    # timestamp column\n",
    "    if \"ts\" in df.columns:\n",
    "        df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
    "\n",
    "    # keep only valid rows\n",
    "    df = df.dropna(subset=[\"ts\", \"ArtifactCorrectedRR\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7c9fd",
   "metadata": {},
   "source": [
    "### Helpers to parse PsychoPy timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a233c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_exptimestamp(s: str) -> datetime:\n",
    "    \"\"\"\n",
    "    Convert PsychoPy string:\n",
    "      '2025-11-12 14h50.45.373178 +0100'\n",
    "    to timezone-aware datetime.\n",
    "    \"\"\"\n",
    "    date, time_part, tz = s.split(\" \")\n",
    "    time_part = time_part.replace(\"h\", \":\")\n",
    "    time_part = time_part.replace(\".\", \":\", 1)  # first '.' -> ':'\n",
    "    fixed = f\"{date} {time_part} {tz}\"\n",
    "    return datetime.strptime(fixed, \"%Y-%m-%d %H:%M:%S.%f %z\")\n",
    "\n",
    "def json_time_to_naive(s: str):\n",
    "    \"\"\"Return naive datetime (no tz) or None if s is None.\"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    dt = parse_exptimestamp(s)\n",
    "    return dt.replace(tzinfo=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ebaf5",
   "metadata": {},
   "source": [
    "### Load HR data for all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59bcbcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HR data for participant 1: 1065 rows\n",
      "Loaded HR data for participant 10: 966 rows\n",
      "Loaded HR data for participant 11: 1490 rows\n",
      "Loaded HR data for participant 12: 1469 rows\n",
      "Loaded HR data for participant 13: 1570 rows\n",
      "Loaded HR data for participant 14: 1288 rows\n",
      "Loaded HR data for participant 15: 1642 rows\n",
      "Loaded HR data for participant 16: 971 rows\n",
      "Loaded HR data for participant 17: 971 rows\n",
      "Loaded HR data for participant 18: 1154 rows\n",
      "Loaded HR data for participant 19: 1154 rows\n",
      "Loaded HR data for participant 2: 914 rows\n",
      "Loaded HR data for participant 20: 971 rows\n",
      "Loaded HR data for participant 21: 897 rows\n",
      "Loaded HR data for participant 22: 1066 rows\n",
      "Loaded HR data for participant 23: 1152 rows\n",
      "Loaded HR data for participant 3: 1065 rows\n",
      "Loaded HR data for participant 4: 1268 rows\n",
      "Loaded HR data for participant 5: 1172 rows\n",
      "Loaded HR data for participant 6: 1254 rows\n",
      "Loaded HR data for participant 7: 1176 rows\n",
      "Loaded HR data for participant 8: 817 rows\n",
      "HR participants available: [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "hr_data = {}\n",
    "\n",
    "for path in glob(\"../trials_hr/participant*.csv\"):\n",
    "    base = os.path.basename(path)              # e.g. 'participant1.csv'\n",
    "    num_str = base.replace(\"participant\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    try:\n",
    "        pid = int(num_str)\n",
    "    except ValueError:\n",
    "        continue  # ignore any weird filenames\n",
    "\n",
    "    df_hr = load_hr_file(path)\n",
    "    hr_data[pid] = df_hr\n",
    "    print(f\"Loaded HR data for participant {pid}: {len(df_hr)} rows\")\n",
    "\n",
    "print(\"HR participants available:\", sorted(hr_data.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244eaaf",
   "metadata": {},
   "source": [
    "### Match HR rows to each video & save new JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fb6696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant 1, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 139 HR rows\n",
      "Participant 1, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 149 HR rows\n",
      "Participant 1, stim 3 (18840567-hd_1920_1080_30fps.mp4): 129 HR rows\n",
      "Participant 1, stim 4 (11946387_3840_2160_30fps.mp4): 150 HR rows\n",
      "Participant 2, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 140 HR rows\n",
      "Participant 2, stim 2 (11946387_3840_2160_30fps.mp4): 148 HR rows\n",
      "Participant 2, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 154 HR rows\n",
      "Participant 2, stim 4 (18840567-hd_1920_1080_30fps.mp4): 114 HR rows\n",
      "Participant 3, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 138 HR rows\n",
      "Participant 3, stim 2 (11946387_3840_2160_30fps.mp4): 149 HR rows\n",
      "Participant 3, stim 3 (18840567-hd_1920_1080_30fps.mp4): 130 HR rows\n",
      "Participant 3, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 150 HR rows\n",
      "Participant 4, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 140 HR rows\n",
      "Participant 4, stim 2 (18840567-hd_1920_1080_30fps.mp4): 132 HR rows\n",
      "Participant 4, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 149 HR rows\n",
      "Participant 4, stim 4 (11946387_3840_2160_30fps.mp4): 152 HR rows\n",
      "Participant 5, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 136 HR rows\n",
      "Participant 5, stim 2 (18840567-hd_1920_1080_30fps.mp4): 132 HR rows\n",
      "Participant 5, stim 3 (11946387_3840_2160_30fps.mp4): 152 HR rows\n",
      "Participant 5, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 150 HR rows\n",
      "Participant 6, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 196 HR rows\n",
      "Participant 6, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 144 HR rows\n",
      "Participant 6, stim 3 (11946387_3840_2160_30fps.mp4): 152 HR rows\n",
      "Participant 6, stim 4 (18840567-hd_1920_1080_30fps.mp4): 146 HR rows\n",
      "Participant 7, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 134 HR rows\n",
      "Participant 7, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 115 HR rows\n",
      "Participant 7, stim 3 (18840567-hd_1920_1080_30fps.mp4): 104 HR rows\n",
      "Participant 7, stim 4 (11946387_3840_2160_30fps.mp4): 128 HR rows\n",
      "Participant 8, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 163 HR rows\n",
      "Participant 8, stim 2 (11946387_3840_2160_30fps.mp4): 151 HR rows\n",
      "Participant 8, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 138 HR rows\n",
      "Participant 8, stim 4 (18840567-hd_1920_1080_30fps.mp4): 56 HR rows\n",
      "Participant 10, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 150 HR rows\n",
      "Participant 10, stim 2 (18840567-hd_1920_1080_30fps.mp4): 131 HR rows\n",
      "Participant 10, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 141 HR rows\n",
      "Participant 10, stim 4 (11946387_3840_2160_30fps.mp4): 150 HR rows\n",
      "Participant 11, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 148 HR rows\n",
      "Participant 11, stim 2 (18840567-hd_1920_1080_30fps.mp4): 133 HR rows\n",
      "Participant 11, stim 3 (11946387_3840_2160_30fps.mp4): 161 HR rows\n",
      "Participant 11, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 120 HR rows\n",
      "Participant 12, stim 1 (11946387_3840_2160_30fps.mp4): 204 HR rows\n",
      "Participant 12, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 176 HR rows\n",
      "Participant 12, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 188 HR rows\n",
      "Participant 12, stim 4 (18840567-hd_1920_1080_30fps.mp4): 167 HR rows\n",
      "Participant 13, stim 1 (11946387_3840_2160_30fps.mp4): 208 HR rows\n",
      "Participant 13, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 182 HR rows\n",
      "Participant 13, stim 3 (18840567-hd_1920_1080_30fps.mp4): 168 HR rows\n",
      "Participant 13, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 203 HR rows\n",
      "Participant 14, stim 1 (11946387_3840_2160_30fps.mp4): 186 HR rows\n",
      "Participant 14, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 188 HR rows\n",
      "Participant 14, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 174 HR rows\n",
      "Participant 14, stim 4 (18840567-hd_1920_1080_30fps.mp4): 161 HR rows\n",
      "Participant 15, stim 1 (11946387_3840_2160_30fps.mp4): 162 HR rows\n",
      "Participant 15, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 177 HR rows\n",
      "Participant 15, stim 3 (18840567-hd_1920_1080_30fps.mp4): 150 HR rows\n",
      "Participant 15, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 155 HR rows\n",
      "Participant 16, stim 1 (11946387_3840_2160_30fps.mp4): 137 HR rows\n",
      "Participant 16, stim 2 (18840567-hd_1920_1080_30fps.mp4): 119 HR rows\n",
      "Participant 16, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 126 HR rows\n",
      "Participant 16, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 138 HR rows\n",
      "Participant 17, stim 1 (11946387_3840_2160_30fps.mp4): 137 HR rows\n",
      "Participant 17, stim 2 (18840567-hd_1920_1080_30fps.mp4): 122 HR rows\n",
      "Participant 17, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 145 HR rows\n",
      "Participant 17, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 125 HR rows\n",
      "Participant 18, stim 1 (18840567-hd_1920_1080_30fps.mp4): 129 HR rows\n",
      "Participant 18, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 125 HR rows\n",
      "Participant 18, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 142 HR rows\n",
      "Participant 18, stim 4 (11946387_3840_2160_30fps.mp4): 195 HR rows\n",
      "Participant 19, stim 1 (18840567-hd_1920_1080_30fps.mp4): 127 HR rows\n",
      "Participant 19, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 127 HR rows\n",
      "Participant 19, stim 3 (11946387_3840_2160_30fps.mp4): 139 HR rows\n",
      "Participant 19, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 183 HR rows\n",
      "Participant 20, stim 1 (18840567-hd_1920_1080_30fps.mp4): 122 HR rows\n",
      "Participant 20, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 135 HR rows\n",
      "Participant 20, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 132 HR rows\n",
      "Participant 20, stim 4 (11946387_3840_2160_30fps.mp4): 139 HR rows\n",
      "Participant 21, stim 1 (18840567-hd_1920_1080_30fps.mp4): 118 HR rows\n",
      "Participant 21, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 139 HR rows\n",
      "Participant 21, stim 3 (11946387_3840_2160_30fps.mp4): 135 HR rows\n",
      "Participant 21, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 127 HR rows\n",
      "Participant 22, stim 1 (18840567-hd_1920_1080_30fps.mp4): 123 HR rows\n",
      "Participant 22, stim 2 (11946387_3840_2160_30fps.mp4): 137 HR rows\n",
      "Participant 22, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 131 HR rows\n",
      "Participant 22, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 163 HR rows\n",
      "Participant 23, stim 1 (18840567-hd_1920_1080_30fps.mp4): 137 HR rows\n",
      "Participant 23, stim 2 (11946387_3840_2160_30fps.mp4): 152 HR rows\n",
      "Participant 23, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 163 HR rows\n",
      "Participant 23, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 155 HR rows\n",
      "\n",
      "Saved merged data with HR to experiment_data_with_hr.json\n"
     ]
    }
   ],
   "source": [
    "# Work on a copy of the participants structure\n",
    "participants_with_hr = all_participants\n",
    "\n",
    "for p in participants_with_hr:\n",
    "    pid = p[\"participant_id\"]\n",
    "    df_hr = hr_data.get(pid)\n",
    "\n",
    "    if df_hr is None:\n",
    "        print(f\"No HR data for participant {pid}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # ensure timestamps are clean and sorted\n",
    "    df_hr[\"ts\"] = pd.to_datetime(df_hr[\"ts\"], errors=\"coerce\")\n",
    "    df_hr = df_hr.dropna(subset=[\"ts\"]).sort_values(\"ts\")\n",
    "\n",
    "    for stim in p[\"stimuli\"]:\n",
    "        v_start = json_time_to_naive(stim.get(\"video_start\"))\n",
    "        v_end   = json_time_to_naive(stim.get(\"video_end\"))\n",
    "\n",
    "        if v_start is None or v_end is None:\n",
    "            stim[\"heart_rate\"] = []\n",
    "            continue\n",
    "\n",
    "        mask = (df_hr[\"ts\"] >= v_start) & (df_hr[\"ts\"] <= v_end)\n",
    "        seg = df_hr.loc[mask, [\"ts\", \"RR\", \"ArtifactCorrectedRR\", \"RawArtifact\"]].copy()\n",
    "\n",
    "        # store timestamp as string so JSON can serialize it\n",
    "        seg[\"ts\"] = seg[\"ts\"].dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        stim[\"heart_rate\"] = seg.to_dict(orient=\"records\")\n",
    "\n",
    "        print(\n",
    "            f\"Participant {pid}, stim {stim['id']} \"\n",
    "            f\"({stim['video_id']}): {len(seg)} HR rows\"\n",
    "        )\n",
    "\n",
    "# finally, save the merged structures\n",
    "output_path = \"experiment_data_with_hr.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(participants_with_hr, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nSaved merged data with HR to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ActiveML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
