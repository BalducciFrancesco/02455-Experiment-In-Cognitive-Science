{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4659a7",
   "metadata": {},
   "source": [
    "## Imports + robust HR loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f04c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_hr_file(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a participantN.csv HR file into a tidy DataFrame with columns:\n",
    "    ts (datetime), RR, ArtifactCorrectedRR, RawArtifact.\n",
    "    Works even if header isn't on the first line.\n",
    "    \"\"\"\n",
    "    lines = Path(path).read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "    # find header line (must contain both 'RR' and 'ts')\n",
    "    header_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"RR\" in line and \"ts\" in line:\n",
    "            header_idx = i\n",
    "            break\n",
    "    if header_idx is None:\n",
    "        raise ValueError(f\"Could not find header with 'RR' and 'ts' in {path}\")\n",
    "\n",
    "    header_line = lines[header_idx].strip()\n",
    "\n",
    "    # detect delimiter\n",
    "    if \";\" in header_line and \",\" not in header_line:\n",
    "        delim = \";\"\n",
    "    else:\n",
    "        delim = \",\"  # default\n",
    "\n",
    "    header = [h.strip() for h in header_line.split(delim)]\n",
    "\n",
    "    data_rows = []\n",
    "    for line in lines[header_idx + 1:]:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        parts = [p.strip() for p in line.split(delim)]\n",
    "        if len(parts) != len(header):\n",
    "            continue\n",
    "        data_rows.append(parts)\n",
    "\n",
    "    df = pd.DataFrame(data_rows, columns=header)\n",
    "\n",
    "    # numeric columns\n",
    "    for col in [\"RR\", \"ArtifactCorrectedRR\", \"RawArtifact\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    if \"RawArtifact\" in df.columns:\n",
    "        df[\"RawArtifact\"] = df[\"RawArtifact\"].fillna(0).astype(int)\n",
    "\n",
    "    # timestamp column\n",
    "    if \"ts\" in df.columns:\n",
    "        df[\"ts\"] = pd.to_datetime(df[\"ts\"], errors=\"coerce\")\n",
    "\n",
    "    # keep only valid rows\n",
    "    df = df.dropna(subset=[\"ts\", \"ArtifactCorrectedRR\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df868e28",
   "metadata": {},
   "source": [
    "## Helpers to parse PsychoPy timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4046bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def parse_exptimestamp(s: str) -> datetime:\n",
    "    \"\"\"\n",
    "    Convert PsychoPy string:\n",
    "      '2025-11-12 14h50.45.373178 +0100'\n",
    "    to timezone-aware datetime.\n",
    "    \"\"\"\n",
    "    date, time_part, tz = s.split(\" \")\n",
    "    time_part = time_part.replace(\"h\", \":\")\n",
    "    time_part = time_part.replace(\".\", \":\", 1)  # first '.' -> ':'\n",
    "    fixed = f\"{date} {time_part} {tz}\"\n",
    "    return datetime.strptime(fixed, \"%Y-%m-%d %H:%M:%S.%f %z\")\n",
    "\n",
    "def json_time_to_naive(s: str):\n",
    "    \"\"\"Return naive datetime (no tz) or None if s is None.\"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    dt = parse_exptimestamp(s)\n",
    "    return dt.replace(tzinfo=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff145c9",
   "metadata": {},
   "source": [
    "## Load experiment JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7f119f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22 participants from JSON\n"
     ]
    }
   ],
   "source": [
    "with open(\"experiment_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    participants = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(participants)} participants from JSON\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e768360",
   "metadata": {},
   "source": [
    "## Load HR data for all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a682ca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HR data for participant 6: 1254 rows\n",
      "Loaded HR data for participant 11: 1490 rows\n",
      "Loaded HR data for participant 10: 966 rows\n",
      "Loaded HR data for participant 7: 1176 rows\n",
      "Loaded HR data for participant 5: 1172 rows\n",
      "Loaded HR data for participant 12: 1469 rows\n",
      "Loaded HR data for participant 13: 1570 rows\n",
      "Loaded HR data for participant 4: 1268 rows\n",
      "Loaded HR data for participant 17: 971 rows\n",
      "Loaded HR data for participant 1: 1065 rows\n",
      "Loaded HR data for participant 16: 971 rows\n",
      "Loaded HR data for participant 14: 1288 rows\n",
      "Loaded HR data for participant 3: 1065 rows\n",
      "Loaded HR data for participant 2: 914 rows\n",
      "Loaded HR data for participant 15: 1642 rows\n",
      "Loaded HR data for participant 18: 1154 rows\n",
      "Loaded HR data for participant 19: 1154 rows\n",
      "Loaded HR data for participant 22: 1066 rows\n",
      "Loaded HR data for participant 8: 817 rows\n",
      "Loaded HR data for participant 23: 1152 rows\n",
      "Loaded HR data for participant 21: 897 rows\n",
      "Loaded HR data for participant 20: 971 rows\n",
      "HR participants available: [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "hr_data = {}\n",
    "\n",
    "for path in glob(\"../trials_hr/participant*.csv\"):\n",
    "    base = os.path.basename(path)              # e.g. 'participant1.csv'\n",
    "    num_str = base.replace(\"participant\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    try:\n",
    "        pid = int(num_str)\n",
    "    except ValueError:\n",
    "        continue  # ignore any weird filenames\n",
    "\n",
    "    df_hr = load_hr_file(path)\n",
    "    hr_data[pid] = df_hr\n",
    "    print(f\"Loaded HR data for participant {pid}: {len(df_hr)} rows\")\n",
    "\n",
    "print(\"HR participants available:\", sorted(hr_data.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710411a",
   "metadata": {},
   "source": [
    "## Match HR rows to each video & save new JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92d242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant 1, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 139 HR rows\n",
      "Participant 1, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 149 HR rows\n",
      "Participant 1, stim 3 (18840567-hd_1920_1080_30fps.mp4): 129 HR rows\n",
      "Participant 1, stim 4 (11946387_3840_2160_30fps.mp4): 150 HR rows\n",
      "Participant 2, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 140 HR rows\n",
      "Participant 2, stim 2 (11946387_3840_2160_30fps.mp4): 148 HR rows\n",
      "Participant 2, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 154 HR rows\n",
      "Participant 2, stim 4 (18840567-hd_1920_1080_30fps.mp4): 114 HR rows\n",
      "Participant 3, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 138 HR rows\n",
      "Participant 3, stim 2 (11946387_3840_2160_30fps.mp4): 149 HR rows\n",
      "Participant 3, stim 3 (18840567-hd_1920_1080_30fps.mp4): 130 HR rows\n",
      "Participant 3, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 150 HR rows\n",
      "Participant 4, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 140 HR rows\n",
      "Participant 4, stim 2 (18840567-hd_1920_1080_30fps.mp4): 132 HR rows\n",
      "Participant 4, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 149 HR rows\n",
      "Participant 4, stim 4 (11946387_3840_2160_30fps.mp4): 152 HR rows\n",
      "Participant 5, stim 1 (4171487-uhd_3840_2160_30fps.mp4): 136 HR rows\n",
      "Participant 5, stim 2 (18840567-hd_1920_1080_30fps.mp4): 132 HR rows\n",
      "Participant 5, stim 3 (11946387_3840_2160_30fps.mp4): 152 HR rows\n",
      "Participant 5, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 150 HR rows\n",
      "Participant 6, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 196 HR rows\n",
      "Participant 6, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 144 HR rows\n",
      "Participant 6, stim 3 (11946387_3840_2160_30fps.mp4): 152 HR rows\n",
      "Participant 6, stim 4 (18840567-hd_1920_1080_30fps.mp4): 146 HR rows\n",
      "Participant 7, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 134 HR rows\n",
      "Participant 7, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 115 HR rows\n",
      "Participant 7, stim 3 (18840567-hd_1920_1080_30fps.mp4): 104 HR rows\n",
      "Participant 7, stim 4 (11946387_3840_2160_30fps.mp4): 128 HR rows\n",
      "Participant 8, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 163 HR rows\n",
      "Participant 8, stim 2 (11946387_3840_2160_30fps.mp4): 151 HR rows\n",
      "Participant 8, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 138 HR rows\n",
      "Participant 8, stim 4 (18840567-hd_1920_1080_30fps.mp4): 56 HR rows\n",
      "Participant 10, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 150 HR rows\n",
      "Participant 10, stim 2 (18840567-hd_1920_1080_30fps.mp4): 131 HR rows\n",
      "Participant 10, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 141 HR rows\n",
      "Participant 10, stim 4 (11946387_3840_2160_30fps.mp4): 150 HR rows\n",
      "Participant 11, stim 1 (5768645-uhd_3840_2160_25fps.mp4): 148 HR rows\n",
      "Participant 11, stim 2 (18840567-hd_1920_1080_30fps.mp4): 133 HR rows\n",
      "Participant 11, stim 3 (11946387_3840_2160_30fps.mp4): 161 HR rows\n",
      "Participant 11, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 120 HR rows\n",
      "Participant 12, stim 1 (11946387_3840_2160_30fps.mp4): 204 HR rows\n",
      "Participant 12, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 176 HR rows\n",
      "Participant 12, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 188 HR rows\n",
      "Participant 12, stim 4 (18840567-hd_1920_1080_30fps.mp4): 167 HR rows\n",
      "Participant 13, stim 1 (11946387_3840_2160_30fps.mp4): 208 HR rows\n",
      "Participant 13, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 182 HR rows\n",
      "Participant 13, stim 3 (18840567-hd_1920_1080_30fps.mp4): 168 HR rows\n",
      "Participant 13, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 203 HR rows\n",
      "Participant 14, stim 1 (11946387_3840_2160_30fps.mp4): 186 HR rows\n",
      "Participant 14, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 188 HR rows\n",
      "Participant 14, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 174 HR rows\n",
      "Participant 14, stim 4 (18840567-hd_1920_1080_30fps.mp4): 161 HR rows\n",
      "Participant 15, stim 1 (11946387_3840_2160_30fps.mp4): 162 HR rows\n",
      "Participant 15, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 177 HR rows\n",
      "Participant 15, stim 3 (18840567-hd_1920_1080_30fps.mp4): 150 HR rows\n",
      "Participant 15, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 155 HR rows\n",
      "Participant 16, stim 1 (11946387_3840_2160_30fps.mp4): 137 HR rows\n",
      "Participant 16, stim 2 (18840567-hd_1920_1080_30fps.mp4): 119 HR rows\n",
      "Participant 16, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 126 HR rows\n",
      "Participant 16, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 138 HR rows\n",
      "Participant 17, stim 1 (11946387_3840_2160_30fps.mp4): 137 HR rows\n",
      "Participant 17, stim 2 (18840567-hd_1920_1080_30fps.mp4): 122 HR rows\n",
      "Participant 17, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 145 HR rows\n",
      "Participant 17, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 125 HR rows\n",
      "Participant 18, stim 1 (18840567-hd_1920_1080_30fps.mp4): 129 HR rows\n",
      "Participant 18, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 125 HR rows\n",
      "Participant 18, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 142 HR rows\n",
      "Participant 18, stim 4 (11946387_3840_2160_30fps.mp4): 195 HR rows\n",
      "Participant 19, stim 1 (18840567-hd_1920_1080_30fps.mp4): 127 HR rows\n",
      "Participant 19, stim 2 (4171487-uhd_3840_2160_30fps.mp4): 127 HR rows\n",
      "Participant 19, stim 3 (11946387_3840_2160_30fps.mp4): 139 HR rows\n",
      "Participant 19, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 183 HR rows\n",
      "Participant 20, stim 1 (18840567-hd_1920_1080_30fps.mp4): 122 HR rows\n",
      "Participant 20, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 135 HR rows\n",
      "Participant 20, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 132 HR rows\n",
      "Participant 20, stim 4 (11946387_3840_2160_30fps.mp4): 139 HR rows\n",
      "Participant 21, stim 1 (18840567-hd_1920_1080_30fps.mp4): 118 HR rows\n",
      "Participant 21, stim 2 (5768645-uhd_3840_2160_25fps.mp4): 139 HR rows\n",
      "Participant 21, stim 3 (11946387_3840_2160_30fps.mp4): 135 HR rows\n",
      "Participant 21, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 127 HR rows\n",
      "Participant 22, stim 1 (18840567-hd_1920_1080_30fps.mp4): 123 HR rows\n",
      "Participant 22, stim 2 (11946387_3840_2160_30fps.mp4): 137 HR rows\n",
      "Participant 22, stim 3 (4171487-uhd_3840_2160_30fps.mp4): 131 HR rows\n",
      "Participant 22, stim 4 (5768645-uhd_3840_2160_25fps.mp4): 163 HR rows\n",
      "Participant 23, stim 1 (18840567-hd_1920_1080_30fps.mp4): 137 HR rows\n",
      "Participant 23, stim 2 (11946387_3840_2160_30fps.mp4): 152 HR rows\n",
      "Participant 23, stim 3 (5768645-uhd_3840_2160_25fps.mp4): 163 HR rows\n",
      "Participant 23, stim 4 (4171487-uhd_3840_2160_30fps.mp4): 155 HR rows\n",
      "\n",
      "Saved merged data with HR to experiment_with_heart_rate.json\n"
     ]
    }
   ],
   "source": [
    "# Work on a copy of the participants structure\n",
    "participants_with_hr = participants\n",
    "\n",
    "for p in participants_with_hr:\n",
    "    pid = p[\"participant_id\"]\n",
    "    df_hr = hr_data.get(pid)\n",
    "\n",
    "    if df_hr is None:\n",
    "        print(f\"No HR data for participant {pid}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # ensure timestamps are clean and sorted\n",
    "    df_hr[\"ts\"] = pd.to_datetime(df_hr[\"ts\"], errors=\"coerce\")\n",
    "    df_hr = df_hr.dropna(subset=[\"ts\"]).sort_values(\"ts\")\n",
    "\n",
    "    for stim in p[\"stimuli\"]:\n",
    "        v_start = json_time_to_naive(stim.get(\"video_start\"))\n",
    "        v_end   = json_time_to_naive(stim.get(\"video_end\"))\n",
    "\n",
    "        if v_start is None or v_end is None:\n",
    "            stim[\"heart_rate\"] = []\n",
    "            continue\n",
    "\n",
    "        mask = (df_hr[\"ts\"] >= v_start) & (df_hr[\"ts\"] <= v_end)\n",
    "        seg = df_hr.loc[mask, [\"ts\", \"RR\", \"ArtifactCorrectedRR\", \"RawArtifact\"]].copy()\n",
    "\n",
    "        # store timestamp as string so JSON can serialize it\n",
    "        seg[\"ts\"] = seg[\"ts\"].dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "        stim[\"heart_rate\"] = seg.to_dict(orient=\"records\")\n",
    "\n",
    "        print(\n",
    "            f\"Participant {pid}, stim {stim['id']} \"\n",
    "            f\"({stim['video_id']}): {len(seg)} HR rows\"\n",
    "        )\n",
    "\n",
    "# finally, save the merged structure\n",
    "output_path = \"experiment_with_heart_rate.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(participants_with_hr, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nSaved merged data with HR to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02450",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
